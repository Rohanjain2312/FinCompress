{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-04-01",
   "metadata": {},
   "source": "# FinCompress — Notebook 04: Structured Pruning\n\n**RUN ON: Colab/GPU**\n\n## Structured vs. Unstructured Pruning\n\nPruning removes unimportant weights from a neural network. There are two families:\n\n**Unstructured pruning** sets individual weights to zero. The result is a *sparse* weight matrix with the same shape. Standard matrix multiplication has no way to skip zero entries — it runs the full computation anyway. Real latency speedup requires specialized sparse BLAS kernels that are not available in standard PyTorch.\n\n**Structured pruning** removes entire structural units: in transformers, this means whole attention heads or entire FFN neurons. The result is a smaller *dense* matrix that is genuinely faster on all hardware without any specialized kernels.\n\n## Why BERT is Overparameterized for Downstream Tasks\n\nBERT was designed for general language understanding — it needs all 12 layers and 12 heads to handle diverse tasks. For a specific 3-class sentiment task on financial text, many of these heads become redundant. Studies show that 30-50% of heads can be removed with minimal accuracy loss on classification tasks (Michel et al., NeurIPS 2019).\n\n## Attention Head Importance Score\n\nWe use entropy-based importance: a head with focused attention patterns (low entropy) is more important than one that attends uniformly (high entropy). Score = 1 - normalized_entropy. Cheap to compute, no gradient needed, correlates well with gradient-based importance in practice."
  },
  {
   "cell_type": "code",
   "id": "cell-04-02",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import os, sys\nPROJECT_PATH = '/content/drive/MyDrive/fincompress'\nos.chdir(PROJECT_PATH)\nsys.path.insert(0, PROJECT_PATH)"
  },
  {
   "cell_type": "code",
   "id": "cell-04-03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Run iterative pruning with recovery fine-tuning\n!python -m fincompress.pruning.prune_finetune"
  },
  {
   "cell_type": "code",
   "id": "cell-04-04",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Head importance heatmap BEFORE pruning\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport pandas as pd\nfrom fincompress.pruning.structured_pruning import get_teacher_head_importance_proxy\n\n# Load teacher\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = AutoTokenizer.from_pretrained('fincompress/checkpoints/teacher/tokenizer')\nteacher = AutoModelForSequenceClassification.from_pretrained('fincompress/checkpoints/teacher').to(device)\nteacher.eval()\n\n# Simple dataset for calibration\nclass DS(Dataset):\n    def __init__(self, df, tok, ml):\n        self.texts = df['text'].tolist(); self.labels = df['label'].tolist()\n        self.tok = tok; self.ml = ml\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        e = self.tok(self.texts[i], max_length=self.ml, padding='max_length', truncation=True, return_tensors='pt')\n        return {'input_ids': e['input_ids'].squeeze(0), 'attention_mask': e['attention_mask'].squeeze(0),\n                'token_type_ids': e.get('token_type_ids', torch.zeros(self.ml, dtype=torch.long)).squeeze(0), 'label': torch.tensor(self.labels[i])}\n\ndf_val = pd.read_csv('fincompress/data/val.csv')\nval_loader = DataLoader(DS(df_val, tokenizer, 128), batch_size=32)\n\n# Compute importance\nimportance = torch.zeros(teacher.config.num_hidden_layers, teacher.config.num_attention_heads)\ncount = 0\nwith torch.no_grad():\n    for bi, batch in enumerate(val_loader):\n        if bi >= 30: break\n        out = teacher(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device), output_attentions=True)\n        seq = batch['input_ids'].shape[1]\n        eps = 1e-9\n        max_ent = torch.log(torch.tensor(seq, dtype=torch.float))\n        for li, aw in enumerate(out.attentions):\n            ent = -(aw * (aw + eps).log()).sum(dim=-1).mean(dim=-1)  # [batch, heads]\n            norm_ent = ent / (max_ent + eps)\n            importance[li] += (1 - norm_ent).mean(dim=0).cpu()\n        count += 1\nimportance /= count\n\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.heatmap(importance.numpy(), annot=True, fmt='.2f', cmap='YlOrRd', ax=ax,\n            xticklabels=[f'H{i}' for i in range(importance.shape[1])],\n            yticklabels=[f'L{i}' for i in range(importance.shape[0])])\nax.set_xlabel('Head Index')\nax.set_ylabel('Layer Index')\nax.set_title('Attention Head Importance Scores (pre-pruning)\\n(higher = more important, red = prime pruning candidate)')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "id": "cell-04-05",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Pruning curve plot\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncurve_df = pd.read_csv('fincompress/results/pruning_curve.csv')\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(curve_df['heads_pruned_pct'], curve_df['val_f1'], 'o-', color='steelblue', linewidth=2)\n\n# Find 'cliff': first point where delta F1 < -0.02 from peak\npeak_f1 = curve_df['val_f1'].max()\ncliff_row = curve_df[curve_df['val_f1'] < peak_f1 - 0.02]\nif not cliff_row.empty:\n    cliff_x = cliff_row.iloc[0]['heads_pruned_pct']\n    ax.axvline(x=cliff_x, color='red', linestyle='--', label=f'Accuracy cliff ({cliff_x:.0f}%)')\n    ax.axvspan(cliff_x, curve_df['heads_pruned_pct'].max() + 5, alpha=0.15, color='red')\n    ax.annotate('Accuracy cliff', xy=(cliff_x, peak_f1 - 0.015), xytext=(cliff_x + 3, peak_f1 - 0.005),\n                arrowprops=dict(arrowstyle='->', color='red'), color='red')\n\nax.set_xlabel('Cumulative Heads Pruned (%)')\nax.set_ylabel('Val Macro F1')\nax.set_title('Validation F1 vs. Attention Head Sparsity\\n(red region = accuracy cliff)')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "id": "cell-04-06",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Pruning summary: which heads were pruned first, which layers most robust\nimport pandas as pd\n\ncurve_df = pd.read_csv('fincompress/results/pruning_curve.csv')\nprint('Pruning Curve Summary')\nprint('=' * 50)\nprint(curve_df.to_string(index=False))\nprint()\nprint(f'Peak val F1:  {curve_df[\"val_f1\"].max():.4f} (at {curve_df.loc[curve_df[\"val_f1\"].idxmax(), \"heads_pruned_pct\"]:.0f}% sparsity)')\nprint(f'Final val F1: {curve_df[\"val_f1\"].iloc[-1]:.4f} (at {curve_df[\"heads_pruned_pct\"].iloc[-1]:.0f}% sparsity)')\nprint(f'Total F1 drop: {curve_df[\"val_f1\"].max() - curve_df[\"val_f1\"].iloc[-1]:+.4f}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-04-07",
   "metadata": {},
   "source": "## Interpretation: What Does It Mean That Certain Heads Are Prunable?\n\nIf 30-50% of attention heads can be removed with minimal accuracy loss, those heads were:\n\n1. **Redundant**: Multiple heads in the same layer attending to similar patterns — only one is needed\n2. **Task-irrelevant**: Heads useful for general language understanding but not sentiment classification\n3. **Overfit to pre-training**: Heads specialized for masked language modeling tasks that don't transfer to sentiment\n\nThe heads that survive pruning tend to be:\n- **Lower layers**: Syntactic patterns (subject-verb agreement, negation scope) that are fundamental to sentiment interpretation\n- **Final layer heads**: Task-relevant attention patterns that directly influence the [CLS] representation used for classification\n\nThis pattern confirms that BERT-style models are significantly overparameterized for narrow downstream tasks — a key motivation for the entire compression field."
  }
 ]
}