# FinCompress â€” Compressing FinBERT for Production Inference

[![Live Demo](https://img.shields.io/badge/ğŸ¤—%20Live%20Demo-HF%20Spaces-blue)](https://huggingface.co/spaces/rohanjain2312/FinCompress)
[![Model](https://img.shields.io/badge/ğŸ¤—%20Student%20Weights-HF%20Hub-orange)](https://huggingface.co/rohanjain2312/FinCompress_student)
[![Python](https://img.shields.io/badge/Python-3.11+-blue?logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C?logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

---

## About

**Rohan Jain** â€” MS Machine Learning, University of Maryland

MS ML student at UMD with a background in data science and analytics, focused on applied ML systems and NLP. This project explores a complete model compression pipeline â€” knowledge distillation, INT8 quantization, and structured pruning â€” applied to a domain-specific financial NLP model. The goal: get a production-viable model that actually fits on a CPU, not just one that scores well in a notebook.

| | |
|---|---|
| ğŸ™ GitHub | [github.com/Rohanjain2312](https://github.com/Rohanjain2312) |
| ğŸ¤— HuggingFace | [huggingface.co/rohanjain2312](https://huggingface.co/rohanjain2312) |
| ğŸ’¼ LinkedIn | [linkedin.com/in/jaroh23](https://www.linkedin.com/in/jaroh23/) |
| ğŸ“§ Email | jaroh23@umd.edu |

---

## What It Is

A systematic compression study on **FinBERT** (ProsusAI/finbert â€” BERT-base pre-trained on 4.9B tokens of financial text) for 3-class financial sentiment classification. Five compression techniques are implemented from scratch, each trained end-to-end on Google Colab and benchmarked on identical CPU hardware.

| Try it | |
|---|---|
| ğŸ¤— [HF Spaces â€” no setup required](https://huggingface.co/spaces/rohanjain2312/FinCompress) | Teacher vs. student side-by-side, live benchmark table |
| ğŸ““ [Google Colab â€” full pipeline](notebooks/fincompress_complete.ipynb) | Single notebook: train all 7 variants on a T4 GPU |

---

## Results

Training hardware: Google Colab T4 GPU. Benchmarking: CPU (median latency over 500 runs, 50 warmup).

| Model | Params | Size | Val Macro F1 | vs Teacher |
|-------|--------|------|-------------|------------|
| Teacher (FinBERT fine-tuned) | 109M | 437.9 MB | **0.8876** | baseline |
| Student â€” Vanilla KD | 19M | 76.1 MB | 0.8017 | 5.8Ã— smaller Â· âˆ’8.6 F1 pts |
| Student â€” Intermediate KD | 19M | 76.1 MB | 0.7712 | 5.8Ã— smaller Â· âˆ’11.6 F1 pts |
| Student â€” PTQ (INT8) | 12M | 47.7 MB | 0.7712 | **9.1Ã— smaller** Â· same F1 as FP32 |
| Student â€” QAT (INT8) | 12M | 47.7 MB | 0.7601 | 9.1Ã— smaller |
| Pruned Teacher 30% | 109M | 437.9 MB | **0.8966** | â†‘ beats teacher by +0.9 pts |
| Pruned Teacher 50% | 109M | 437.9 MB | **0.8936** | â†‘ beats teacher by +0.6 pts |

**Surprising finding:** Removing 30â€“50% of attention heads *improved* accuracy. Low-entropy heads (near-uniform attention distributions) add noise rather than signal â€” pruning them acts as structured regularisation. Full latency and throughput numbers are in the executed notebook.

---

## Compression Pipeline

```
                   FinBERT Teacher
                  (12 layers, 768d)
                  ProsusAI/finbert
                  Fine-tune on GPU
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚              â”‚
   Knowledge        Quantization   Structured
   Distillation                     Pruning
         â”‚              â”‚              â”‚
   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚           â”‚   â”‚         â”‚   â”‚         â”‚
 Vanilla   Intermed.  PTQ    QAT  30%     50%
   KD        KD    (INT8) (INT8) pruned  pruned
 (4L/384d) (4L/384d)
         â”‚              â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
               CPU Benchmark (7 variants)
               Median latency Â· Macro F1 Â· Throughput
```

---

## Engineering Concepts Demonstrated

| Concept | Implementation |
|---------|----------------|
| **Knowledge distillation** | Soft-label KL divergence with temperature scaling (T=4) and TÂ² gradient correction; layer-mapped hidden-state MSE + attention-pattern MSE for intermediate KD |
| **INT8 dynamic quantization** | `torch.quantization.quantize_dynamic` (fbgemm backend); QAT with fake-quant ops and straight-through estimator (STE) for gradient-through-discrete-ops |
| **Structured attention pruning** | Per-head entropy scoring over validation set; iterative prune + fine-tune recovery (5 rounds Ã— 3 epochs); importance metric derived from attention distribution uniformity |
| **Custom transformer from scratch** | 4-layer BERT-style encoder in pure `torch.nn` â€” multi-head self-attention, positional + segment + token embeddings, GELU FFN, LayerNorm residuals |
| **Benchmarking protocol** | Median-over-500 CPU latency (not mean â€” right-skewed distribution from GC pauses); 50 warmup runs; throughput measured at batch=32 |
| **End-to-end reproducibility** | Single Colab notebook trains all 7 variants in sequence; `checkpoint_info.json` per model captures hyperparameters + metrics + timestamp |

---

## Techniques Covered

| Technique | Method | Reference |
|-----------|--------|-----------|
| **Vanilla KD** | Soft-label KL loss + CE loss, temperature scaling | [Hinton et al., 2015](https://arxiv.org/abs/1503.02531) |
| **Intermediate KD** | Hidden-state MSE + attention-pattern MSE at mapped layers | [TinyBERT, Jiao et al., 2020](https://arxiv.org/abs/1909.10351) |
| **INT8 PTQ** | Post-training dynamic quantization (fbgemm) | [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html) |
| **INT8 QAT** | Fake-quant + straight-through estimator fine-tuning | [PyTorch QAT Guide](https://pytorch.org/blog/quantization-in-practice/) |
| **Structured Pruning** | Entropy-based head importance, iterative prune + recover | [Michel et al., 2019](https://arxiv.org/abs/1905.10650) |

---

## How to Run

### Colab â€” full pipeline (recommended)

Open [`notebooks/fincompress_complete.ipynb`](notebooks/fincompress_complete.ipynb) in Google Colab with a T4 GPU runtime. The notebook runs the complete pipeline top-to-bottom: dataset prep â†’ teacher training â†’ distillation â†’ quantization â†’ pruning â†’ benchmarking â†’ plots. Runtime: ~3â€“4 hours total.

### Local â€” benchmark only

```bash
git clone https://github.com/Rohanjain2312/FinCompress.git
cd FinCompress/fincompress
pip install -r requirements.txt

# Download checkpoints from Google Drive into fincompress/checkpoints/
python -m fincompress.evaluation.benchmark
```

### Local â€” dataset prep only

```bash
python -m fincompress.data.prepare_dataset
# â†’ fincompress/data/train.csv, val.csv, test.csv
```

---

## Key Learnings

### Knowledge Distillation
- Soft labels encode class uncertainty that hard one-hot labels discard â€” the teacher's [0.05, 0.72, 0.23] output teaches far more than the label "neutral"
- **TÂ² scaling is non-optional:** high temperature flattens the soft distribution, reducing gradient magnitude of the KL term; multiplying by TÂ² restores it â€” without this, CE loss dominates and you lose most of the distillation signal
- Layer mapping strategy matters: evenly-spaced pairing `{0â†’2, 1â†’5, 2â†’8, 3â†’11}` forces the student to mimic the full representational hierarchy (syntactic early layers, semantic middle, task-specific final), not just the output layers

### INT8 Quantization
- PTQ is nearly free â€” apply it as the first compression step before committing to QAT's training cost
- The first and last layers are sensitivity cliffs: quantizing the embedding layer and final classifier causes disproportionate F1 loss; `quantize_dynamic` wisely excludes them by default
- **STE is elegant:** the straight-through estimator propagates gradients through the floor() rounding operation as if it were identity during backprop â€” a simple trick that makes an otherwise non-differentiable step trainable

### Structured Pruning
- 30â€“50% of FinBERT's attention heads are redundant for 3-class financial sentiment â€” entropy scoring identifies them cheaply without computing saliency gradients
- **The regularisation effect is real:** removing redundant heads reduced overfitting enough to improve val F1 by +0.9 pts â€” the original teacher was slightly over-parameterised for this task
- The accuracy cliff is sharp: there is a head-count threshold beyond which each additional pruning round causes rapid F1 collapse; iterative pruning with recovery rounds defers this cliff significantly

---

## Project Structure

```
fincompress/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prepare_dataset.py           Download + merge + split (CPU)
â”œâ”€â”€ teacher/
â”‚   â””â”€â”€ train_teacher.py             Fine-tune FinBERT teacher (Colab/GPU)
â”œâ”€â”€ distillation/
â”‚   â”œâ”€â”€ student_architecture.py      4-layer custom transformer from scratch
â”‚   â”œâ”€â”€ soft_label_distillation.py   Vanilla KD training loop (Colab/GPU)
â”‚   â””â”€â”€ intermediate_distillation.py Hidden + attention KD (Colab/GPU)
â”œâ”€â”€ quantization/
â”‚   â”œâ”€â”€ ptq.py                       Post-training INT8 quantization (CPU)
â”‚   â””â”€â”€ qat.py                       Quantization-aware training (Colab/GPU)
â”œâ”€â”€ pruning/
â”‚   â”œâ”€â”€ structured_pruning.py        Entropy-based head scoring + pruning
â”‚   â””â”€â”€ prune_finetune.py            Iterative prune + recover loop (Colab/GPU)
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ benchmark.py                 Master benchmark â€” 7 variants, CPU
â”œâ”€â”€ checkpoints/                     Gitignored (large binaries on Drive)
â”‚   â””â”€â”€ */checkpoint_info.json       âœ… Committed â€” hyperparams + metrics
â”œâ”€â”€ results/                         benchmark_results.csv/json committed here
â””â”€â”€ logs/                            Training CSVs (gitignored)
notebooks/
â””â”€â”€ fincompress_complete.ipynb       Single notebook â€” full pipeline on Colab
hf_space/
â””â”€â”€ app.py                           Gradio 6 demo â†’ HF Spaces
```

---

## Hardware Requirements

| Task | Hardware | Est. Time |
|------|----------|-----------|
| Dataset prep | Any CPU | ~2 min |
| Teacher fine-tuning | GPU â‰¥ 8 GB VRAM | ~30 min on T4 |
| Vanilla KD | GPU â‰¥ 8 GB VRAM | ~30 min on T4 |
| Intermediate KD | GPU â‰¥ 8 GB VRAM | ~45 min on T4 |
| QAT | GPU â‰¥ 8 GB VRAM | ~15 min on T4 |
| Pruning (both variants) | GPU â‰¥ 8 GB VRAM | ~60 min on T4 |
| PTQ + Benchmarking | CPU (x86) | ~15 min |

Google Colab free tier (T4) is sufficient for all GPU tasks.

---

## References

1. Hinton, G., Vinyals, O., Dean, J. (2015). [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
2. Jiao, X. et al. (2020). [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
3. Michel, P. et al. (2019). [Are Sixteen Heads Really Better Than One?](https://arxiv.org/abs/1905.10650). NeurIPS 2019.
4. Araci, D. (2019). [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)
5. PyTorch Team. [Quantization â€” PyTorch Docs](https://pytorch.org/docs/stable/quantization.html)

---

## Development Notes

The compression pipeline architecture, distillation loss formulations, student architecture design, benchmarking protocol, and all key engineering decisions were designed and authored by Rohan Jain. [Claude Code](https://claude.ai/code) was used as an implementation accelerator for boilerplate, file scaffolding, and debugging â€” similar to how a senior engineer uses Copilot while retaining full design ownership.

---

## License

MIT License â€” see [LICENSE](LICENSE) for details.

---

*Built by Rohan Jain â€” UMD MSML, Spring 2026*
