{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e535e5b98eb4031",
   "metadata": {},
   "source": "# FinCompress — Complete Pipeline Notebook\n**Run on: Google Colab with T4 GPU**\n\nThis single notebook runs the **entire FinCompress pipeline** from start to finish, in one session.\n\n| Section | What it does | Est. Time on T4 |\n|---------|-------------|-----------------|\n| 0. Setup | Mount Drive, install deps | ~5 min |\n| 1. Dataset | Download + split data | ~2 min |\n| 2. Teacher | Fine-tune FinBERT | ~45–60 min |\n| 3a. Vanilla KD | Soft-label knowledge distillation | ~30–45 min |\n| 3b. Intermediate KD | Hidden-state + attention supervision | ~30–45 min |\n| 4a. PTQ | Post-training INT8 quantization | ~5 min |\n| 4b. QAT | Quantization-aware training | ~20–30 min |\n| 5. Pruning | Structured prune + recovery | ~60–90 min |\n| 6. Benchmark | Evaluate all 7 variants | ~10–15 min |\n| 7. Plots | 6 publication-quality figures | ~2 min |\n| 8. Export | ZIP checkpoints for local use | ~2 min |\n\n**Total: ~3–5 hours.** Use Runtime → Run all, then walk away.\n\n> ⚠️ Before starting: **Runtime → Change runtime type → T4 GPU**"
  },
  {
   "cell_type": "markdown",
   "id": "e0704d14c3394677",
   "metadata": {},
   "source": "---\n## Section 0 — Setup\n\nThis cell does everything needed to start: mounts Google Drive, clones (or updates) the repo, installs all Python dependencies, and verifies GPU access.\n\n**Run this cell once.** Every subsequent cell assumes it has completed successfully. If your session disconnects and restarts, just re-run this cell before continuing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4a7dc889d438c",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 0: Setup ─────────────────────────────────────────────────────────\n# Mounts Drive, clones/updates repo, installs deps, checks GPU.\n# Run ONCE per session — every other cell depends on this.\n\nfrom google.colab import drive\nimport os, sys, torch\n\n# Step 1: Mount Google Drive\ndrive.mount('/content/drive')\n\n# Step 2: Define project root — ALL subsequent cells use this variable\nPROJECT_PATH = '/content/drive/MyDrive/fincompress'\n\n# Step 3: Clone repo if not present, otherwise pull latest\nif os.path.exists(os.path.join(PROJECT_PATH, '.git')):\n    print(\"Repo already on Drive — pulling latest changes...\")\n    !git -C {PROJECT_PATH} pull\nelse:\n    print(\"Cloning FinCompress from GitHub...\")\n    !git clone https://github.com/Rohanjain2312/FinCompress.git {PROJECT_PATH}\n\n# Step 4: Change into project directory and add to Python path\nos.chdir(PROJECT_PATH)\nif PROJECT_PATH not in sys.path:\n    sys.path.insert(0, PROJECT_PATH)\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Step 5: Install all dependencies\nprint(\"\\nInstalling dependencies (3–5 min on first run, fast after that)...\")\n!pip install -r requirements_colab.txt -q\nprint(\"Dependencies installed ✓\")\n\n# Step 6: Verify GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nif device == 'cuda':\n    gpu_name = torch.cuda.get_device_name(0)\n    vram_gb  = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"\\n✅ GPU ready: {gpu_name} ({vram_gb:.1f} GB VRAM)\")\nelse:\n    print(\"\\n⚠️  WARNING: No GPU detected.\")\n    print(\"   Go to Runtime → Change runtime type → T4 GPU, then re-run this cell.\")"
  },
  {
   "cell_type": "markdown",
   "id": "1c0d9d3512cf4441",
   "metadata": {},
   "source": "---\n## Section 1 — Dataset Preparation\n\nWe train and evaluate on a merged dataset combining two complementary financial sentiment sources:\n\n| Dataset | Source | Size | Labels |\n|---------|--------|------|--------|\n| **FinancialPhraseBank** (allagree) | Expert-annotated financial news sentences | ~2,264 | negative / neutral / positive |\n| **FiQA-2018 Sentiment** | Crowdsourced financial Q&A with continuous scores | ~1,174 | Continuous [−1, 1] → thresholded |\n\n**Why Macro F1 (not accuracy)?** Financial news is class-imbalanced (~60% neutral). Accuracy rewards predicting \"neutral\" always. Macro F1 weights all three classes equally, penalising models that ignore the minority class.\n\n**Splits:** Stratified 70 / 15 / 15 — each split preserves the same class distribution as the full dataset.\n\n> ⏱️ **~2 minutes** on first run (downloads from HuggingFace Hub). If the CSVs already exist on Drive from a previous run, this cell skips the download automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c231a454440cc",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 1: Dataset Preparation ──────────────────────────────────────────\n# Downloads FinancialPhraseBank + FiQA-2018 from HuggingFace Hub, merges,\n# cleans, and splits 70/15/15 (stratified). Saves train/val/test CSVs to\n# fincompress/data/. Skips download if CSVs already exist on Drive.\n\nimport os, pandas as pd\n\nDATA_DIR  = os.path.join(PROJECT_PATH, 'fincompress', 'data')\ntrain_csv = os.path.join(DATA_DIR, 'train.csv')\n\nif not os.path.exists(train_csv):\n    print(\"Downloading and preparing datasets (2–3 min)...\")\n    !python -m fincompress.data.prepare_dataset\nelse:\n    print(\"Dataset CSVs already on Drive — skipping download.\\n\")\n\ndf_train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\ndf_val   = pd.read_csv(os.path.join(DATA_DIR, 'val.csv'))\ndf_test  = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n\nlabel_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\nprint(\"Class distribution by split:\")\nprint(\"-\" * 45)\nfor split_name, df in [('TRAIN', df_train), ('VAL', df_val), ('TEST', df_test)]:\n    print(f\"\\n{split_name} ({len(df)} samples):\")\n    print(df['label'].map(label_map).value_counts(normalize=True).round(3).to_string())"
  },
  {
   "cell_type": "markdown",
   "id": "f757e3ff8fa548dd",
   "metadata": {},
   "source": "---\n## Section 2 — Teacher Training\n\nWe fine-tune **ProsusAI/FinBERT** — a BERT-base model further pre-trained on 4.9 billion tokens of financial text (news, earnings call transcripts, analyst reports) — as our teacher model.\n\n**Why FinBERT over vanilla BERT?** Financial text has a very different distribution from Wikipedia/BookCorpus. FinBERT provides:\n- Better tokenisation of financial terms (\"EPS beat\", \"EBITDA\" appear as single tokens, not broken subwords)\n- Contextualized representations that already encode domain-specific sentiment signals\n- Higher baseline accuracy on financial NLP *before* any task-specific fine-tuning\n\n**Training strategy:**\n- **Optimizer:** AdamW, lr = 2e-5, weight decay = 0.01 (bias and LayerNorm excluded)\n- **Schedule:** Linear warmup for 10% of steps, then linear decay — prevents destructive gradient updates early in fine-tuning\n- **Early stopping:** Patience = 3 consecutive epochs without val F1 improvement\n- **Target:** Val Macro F1 ≥ 0.87 — below this threshold, the teacher's soft labels don't carry enough structure for effective distillation\n\nThe best checkpoint (by val Macro F1) is saved automatically to `fincompress/checkpoints/teacher/`.\n\n> ⏱️ **~45–60 minutes on T4 GPU.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44acb019943448ee",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 2a: Train Teacher ────────────────────────────────────────────────\n# Fine-tunes FinBERT on the merged financial sentiment dataset.\n# Logs: fincompress/logs/teacher_training.csv\n# Output: fincompress/checkpoints/teacher/  (best by val Macro F1)\n\n!python -m fincompress.teacher.train_teacher"
  },
  {
   "cell_type": "markdown",
   "id": "d70879a1e02f4923",
   "metadata": {},
   "source": "**Reading the plots below:**\n- **Left — Loss curves:** Both train and val loss should decrease together. If val loss rises while train loss keeps falling, the model is overfitting (early stopping will catch this).\n- **Right — Val Macro F1:** The green line should cross the red dashed target line at F1 = 0.87. If it plateaus below 0.87, the teacher isn't strong enough to be an effective distillation target."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e71f4260d040c7",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 2b: Teacher Loss Curves ─────────────────────────────────────────\n# Plots train/val loss and val Macro F1 across epochs.\n# Target: val F1 crosses 0.87 (red dashed line).\n\nimport pandas as pd, matplotlib.pyplot as plt, os\n\nlog_df = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'logs', 'teacher_training.csv'))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(log_df['epoch'], log_df['train_loss'], 'o-', label='Train Loss', color='steelblue')\naxes[0].plot(log_df['epoch'], log_df['val_loss'],   's-', label='Val Loss',   color='coral')\naxes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Cross-Entropy Loss')\naxes[0].set_title('Teacher: Training and Validation Loss')\naxes[0].legend(); axes[0].grid(True, alpha=0.3)\n\naxes[1].plot(log_df['epoch'], log_df['val_f1'], 'o-', color='mediumseagreen', label='Val Macro F1')\naxes[1].axhline(y=0.87, color='red', linestyle='--', label='Target F1 = 0.87')\naxes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Val Macro F1')\naxes[1].set_title('Teacher: Val Macro F1 by Epoch')\naxes[1].legend(); axes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nprint(f\"Best val Macro F1: {log_df['val_f1'].max():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "d07ffc630b2447e6",
   "metadata": {},
   "source": "**Reading the confusion matrix below:**\nThe matrix shows where the teacher makes mistakes on the validation set. Common patterns in financial sentiment:\n- **Negative ↔ Neutral** (most common error): Hedged language like *\"results were below expectations\"* sits close to the neutral boundary in representation space\n- **Positive ↔ Neutral** (less common): Explicit language like *\"record revenues\"* is usually unambiguous\n- **Negative ↔ Positive** (rare): Direct sentiment opposites are linguistically clear\n\nThis tells us what to watch for in compressed variants — accuracy degradation typically shows up first as lower negative recall."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62dd1e31714a96",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 2c: Teacher Confusion Matrix ─────────────────────────────────────\n# Runs the saved teacher on the val set and plots a 3×3 confusion matrix.\n# Shows which sentiment classes the teacher confuses most.\n\nimport torch, os, pandas as pd\nimport matplotlib.pyplot as plt, seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import DataLoader, Dataset\n\nclass FinDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.texts  = df['text'].tolist()\n        self.labels = df['label'].tolist()\n        self.tok = tokenizer; self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], max_length=self.max_len,\n                       padding='max_length', truncation=True, return_tensors='pt')\n        return {'input_ids':      enc['input_ids'].squeeze(0),\n                'attention_mask': enc['attention_mask'].squeeze(0),\n                'label':          torch.tensor(self.labels[i])}\n\ndevice    = 'cuda' if torch.cuda.is_available() else 'cpu'\nckpt_dir  = os.path.join(PROJECT_PATH, 'fincompress', 'checkpoints', 'teacher')\ntokenizer = AutoTokenizer.from_pretrained(os.path.join(ckpt_dir, 'tokenizer'))\nteacher   = AutoModelForSequenceClassification.from_pretrained(ckpt_dir).to(device)\nteacher.eval()\n\ndf_val = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'data', 'val.csv'))\nloader = DataLoader(FinDataset(df_val, tokenizer, 128), batch_size=64)\n\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for batch in loader:\n        out = teacher(batch['input_ids'].to(device),\n                      attention_mask=batch['attention_mask'].to(device))\n        all_preds.extend(out.logits.argmax(dim=-1).cpu().tolist())\n        all_labels.extend(batch['label'].tolist())\n\nlabel_names = ['negative', 'neutral', 'positive']\ncm = confusion_matrix(all_labels, all_preds)\nfig, ax = plt.subplots(figsize=(7, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=label_names, yticklabels=label_names, ax=ax)\nax.set_xlabel('Predicted'); ax.set_ylabel('True')\nax.set_title('Teacher Confusion Matrix (Val Set)')\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "ffb3b8f2c1564c32",
   "metadata": {},
   "source": "---\n## Section 3 — Knowledge Distillation\n\nKnowledge distillation trains a **smaller student model** to mimic the teacher by learning from the teacher's full output probability distributions — not just hard ground-truth labels.\n\n### 3a — Vanilla KD (Soft-Label Distillation)\n\n**Why soft labels?** When the teacher outputs [0.05, 0.72, 0.23] for a sentence, it's telling the student: *\"this is probably neutral, but has some positive signal.\"* A hard label of \"neutral\" discards this nuance entirely. The student trained on soft labels learns the teacher's uncertainty structure, not just its final answer.\n\n**Temperature T:** Softens the distribution before computing KL divergence: `p_i = exp(z_i / T) / Σ exp(z_j / T)`. Higher T → softer distribution → more gradient signal about non-dominant classes.\n\n**T² scaling:** Multiplying the KL loss by T² compensates for the reduced gradient magnitude at high temperatures — without this, the CE loss would dominate.\n\n**Alpha (α):** Balances the two loss terms:\n```\nL = α · T² · KL(student ‖ teacher_soft) + (1 − α) · CE(student, hard_labels)\n```\n\n> ⏱️ **~30–45 minutes on T4 GPU.** Student checkpoint saved to `fincompress/checkpoints/student_vanilla/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4aadd9df9641b6",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 3a-i: Train Vanilla KD Student ───────────────────────────────────\n# Trains a 4-layer, 384-hidden, 6-head student using soft-label KL divergence.\n# Logs: fincompress/logs/vanilla_kd_training.csv\n# Output: fincompress/checkpoints/student_vanilla/\n\n!python -m fincompress.distillation.soft_label_distillation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed696c50e34e31",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 3a-ii: Vanilla KD Loss Curves ────────────────────────────────────\n# Shows how each loss component evolves during training.\n# Total = α·T²·KL + (1−α)·CE. KL should decrease as the student learns\n# to match the teacher's output distributions.\n\nimport pandas as pd, matplotlib.pyplot as plt, os\n\nlog_df = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'logs', 'vanilla_kd_training.csv'))\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(log_df['epoch'], log_df['train_total_loss'], 'o-', color='steelblue',     label='Total Loss')\nax.plot(log_df['epoch'], log_df['train_ce_loss'],    's-', color='coral',          label='CE Loss')\nax.plot(log_df['epoch'], log_df['train_kl_loss'],    '^-', color='mediumseagreen', label='KL Loss (×T²)')\nax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\nax.set_title('Vanilla KD: Training Loss Components')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39928d3de91048f0",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 3a-iii: Teacher vs. Student (Vanilla KD) Comparison ──────────────\n# Compares key metrics between the teacher and the vanilla KD student.\n# The student should be much smaller with a modest F1 drop.\n\nimport json, os\n\ndef load_info(ckpt_dir_name):\n    p = os.path.join(PROJECT_PATH, 'fincompress', 'checkpoints', ckpt_dir_name, 'checkpoint_info.json')\n    with open(p) as f: return json.load(f)\n\nteacher_info = load_info('teacher')\nvanilla_info = load_info('student_vanilla')\n\nrows = [('Teacher (FinBERT, FP32)', teacher_info),\n        ('Student — Vanilla KD',    vanilla_info)]\n\nprint(f\"\\n{'Model':<30} {'Val Macro F1':>14} {'Params (M)':>12} {'Size (MB)':>11}\")\nprint(\"-\" * 70)\nfor name, info in rows:\n    f1     = info.get('best_val_f1', info.get('val_f1', 0))\n    params = info.get('num_params', 0) / 1e6\n    size   = info.get('size_mb', 0)\n    print(f\"{name:<30} {f1:>14.4f} {params:>12.1f} {size:>11.1f}\")\n\nt_f1 = teacher_info.get('best_val_f1', teacher_info.get('val_f1', 0))\nv_f1 = vanilla_info.get('best_val_f1', vanilla_info.get('val_f1', 0))\nt_sz = teacher_info.get('size_mb', 1)\nv_sz = vanilla_info.get('size_mb', 1)\nprint(f\"\\n  F1 gap:             {v_f1 - t_f1:+.4f}\")\nprint(f\"  Size compression:   {t_sz / v_sz:.1f}×\")"
  },
  {
   "cell_type": "markdown",
   "id": "32a5f4f0b3494223",
   "metadata": {},
   "source": "### 3b — Intermediate KD (Hidden States + Attention Supervision)\n\nVanilla KD only supervises the final output logits. The student is free to build entirely different internal representations as long as the final distribution matches. Intermediate distillation adds two additional supervision signals:\n\n1. **Hidden state MSE:** `L_hidden = MSE(proj(student_hidden_i), teacher_hidden_j)` — a trainable linear projection maps the student's 384-dim vectors to 768-dim before comparing with the teacher's hidden states. This forces the student to develop similar intermediate representations.\n\n2. **Attention pattern MSE:** `L_attn = MSE(student_attn_i, teacher_attn_j)` — forces the student's attention heads to focus on the same parts of the sequence as the teacher's corresponding heads.\n\n**Layer mapping** `{student_layer → teacher_layer}` = `{0→2, 1→5, 2→8, 3→11}` — each of the student's 4 layers is paired with an evenly spaced teacher layer, ensuring coverage of early (syntactic), middle (semantic), and final (task-specific) representations.\n\n**Total loss:** `L = α·T²·KL + (1−α)·CE + λ₁·L_hidden + λ₂·L_attn`\n\n> ⏱️ **~30–45 minutes on T4 GPU.** Checkpoint saved to `fincompress/checkpoints/student_intermediate/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7e52416404c2d",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 3b-i: Train Intermediate KD Student ──────────────────────────────\n# Adds hidden state MSE + attention pattern MSE on top of vanilla KD.\n# Logs: fincompress/logs/intermediate_kd_training.csv\n# Output: fincompress/checkpoints/student_intermediate/\n\n!python -m fincompress.distillation.intermediate_distillation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc111df4357d4709",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 3b-ii: Intermediate KD Loss Curves ───────────────────────────────\n# Shows all 5 loss components: total, CE, KL, hidden MSE, attention MSE.\n# Hidden and attention losses should decrease as the student's internals\n# align with the teacher's.\n\nimport pandas as pd, matplotlib.pyplot as plt, os\n\nlog_df = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'logs', 'intermediate_kd_training.csv'))\n\nfig, ax = plt.subplots(figsize=(12, 5))\nstyles = [\n    ('train_total_loss',  'o-', 'steelblue',     'Total Loss'),\n    ('train_ce_loss',     's-', 'coral',          'CE Loss'),\n    ('train_kl_loss',     '^-', 'mediumseagreen', 'KL Loss (×T²)'),\n    ('train_hidden_loss', 'D-', 'darkorange',     'Hidden State MSE'),\n    ('train_attn_loss',   'v-', 'purple',         'Attention Pattern MSE'),\n]\nfor col, style, color, label in styles:\n    if col in log_df.columns:\n        ax.plot(log_df['epoch'], log_df[col], style, color=color, label=label)\n\nax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\nax.set_title('Intermediate KD: Training Loss Components')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182f8b5a9c5480d",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 3b-iii: 3-Way Distillation Comparison ────────────────────────────\n# Compares teacher, vanilla KD student, and intermediate KD student\n# across Val Macro F1, parameter count, and model size.\n\nimport json, os, numpy as np, matplotlib.pyplot as plt\n\ndef load_info(ckpt_dir_name):\n    p = os.path.join(PROJECT_PATH, 'fincompress', 'checkpoints', ckpt_dir_name, 'checkpoint_info.json')\n    with open(p) as f: return json.load(f)\n\ninfos = {\n    'Teacher':          load_info('teacher'),\n    'Vanilla KD':       load_info('student_vanilla'),\n    'Intermediate KD':  load_info('student_intermediate'),\n}\n\nnames  = list(infos.keys())\nf1s    = [v.get('best_val_f1', v.get('val_f1', 0))    for v in infos.values()]\nparams = [v.get('num_params', 0) / 1e6                for v in infos.values()]\nsizes  = [v.get('size_mb', 0)                         for v in infos.values()]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor ax, values, title, color, fmt in zip(\n    axes,\n    [f1s, params, sizes],\n    ['Val Macro F1', 'Parameters (M)', 'Size (MB)'],\n    ['steelblue', 'coral', 'mediumseagreen'],\n    ['.4f', '.1f', '.1f'],\n):\n    bars = ax.bar(names, values, color=color, alpha=0.85, edgecolor='white', width=0.5)\n    ax.bar_label(bars, labels=[f'{v:{fmt}}' for v in values], padding=4, fontsize=10)\n    ax.set_title(title, fontsize=12)\n    ax.set_ylim(0, max(values) * 1.25)\n    ax.grid(True, axis='y', alpha=0.3)\n\nfig.suptitle('Teacher vs. Distilled Students', fontsize=14, fontweight='bold')\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "ab2ad6c94a904fdd",
   "metadata": {},
   "source": "---\n## Section 4 — Quantization\n\nQuantization reduces weight (and optionally activation) precision from 32-bit float (FP32) to 8-bit integer (INT8), cutting model size by ~4× and often speeding up CPU inference via integer arithmetic units.\n\n### 4a — Post-Training Quantization (PTQ)\n\n**Dynamic PTQ** (our approach): weights are statically quantized to INT8; activations are computed in FP32 but dequantized after each linear layer at runtime. No retraining required — the existing weights are simply rescaled.\n\n**Why dynamic (not static)?** The student model uses raw `torch.matmul(Q, K.T)` in its attention code, which doesn't accept pre-quantized tensor inputs. Static PTQ would require refactoring the forward pass.\n\n**Backend: `fbgemm`** — Facebook's General Matrix Multiplication library, optimised for x86 CPUs. Use `qnnpack` for ARM/mobile targets.\n\n**What stays FP32:**\n- **Embeddings** — INT8 lookup tables provide no GEMM speedup\n- **Classifier head** — too small to benefit; keeping FP32 preserves final-layer precision\n\n> ⏱️ **~5 minutes** (no GPU needed). Checkpoint saved to `fincompress/checkpoints/student_ptq/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8687c770af4785",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 4a-i: Run Post-Training Quantization ─────────────────────────────\n# Applies dynamic INT8 quantization to the intermediate KD student.\n# No training required — weights are rescaled and converted.\n# Output: fincompress/checkpoints/student_ptq/\n\n!python -m fincompress.quantization.ptq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eef405a0ae4322",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 4a-ii: FP32 vs. INT8 (PTQ) Comparison ───────────────────────────\n# Shows the accuracy cost and size benefit of INT8 quantization.\n# A F1 drop < 0.02 (2%) is generally acceptable for production use.\n\nimport json, os\n\ndef load_info(ckpt_dir_name):\n    p = os.path.join(PROJECT_PATH, 'fincompress', 'checkpoints', ckpt_dir_name, 'checkpoint_info.json')\n    with open(p) as f: return json.load(f)\n\nfp32_info = load_info('student_intermediate')\nptq_info  = load_info('student_ptq')\n\nfp32_f1, fp32_sz = fp32_info.get('best_val_f1', fp32_info.get('val_f1', 0)), fp32_info.get('size_mb', 0)\nptq_f1,  ptq_sz  = ptq_info.get('best_val_f1', ptq_info.get('val_f1', 0)),   ptq_info.get('size_mb', 0)\n\nprint(f\"\\n{'Model':<35} {'Val Macro F1':>14} {'Size (MB)':>11} {'Compression':>13}\")\nprint(\"-\" * 76)\nprint(f\"{'Student (FP32 — Intermediate KD)':<35} {fp32_f1:>14.4f} {fp32_sz:>11.1f} {'1.0×':>13}\")\nprint(f\"{'Student (INT8 — PTQ)':<35} {ptq_f1:>14.4f} {ptq_sz:>11.1f} {fp32_sz/max(ptq_sz,0.01):>12.1f}×\")\nprint(f\"\\n  F1 change:      {ptq_f1 - fp32_f1:+.4f}\")\nprint(f\"  Size reduction: {(1 - ptq_sz/max(fp32_sz,0.01))*100:.1f}%\")\nprint(f\"  Decision rule:  {'✅ PTQ is acceptable (drop < 2%)' if abs(ptq_f1 - fp32_f1) < 0.02 else '⚠️  PTQ drops > 2% — consider QAT'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "5fddf97a6c0a4a08",
   "metadata": {},
   "source": "### 4b — Quantization-Aware Training (QAT)\n\nQAT inserts **fake-quantization nodes** into the model *during training*. These simulate INT8 rounding in the forward pass so the model learns weights that are robust to quantization, while still allowing real-valued gradients to flow backward (via the **straight-through estimator, STE**: gradients pass through the rounding operation as if it were the identity).\n\nThe model is fine-tuned for 3 epochs with fake-quant active, then converted to true INT8 for inference — recovering 1–2% accuracy lost by PTQ.\n\n**When to use QAT vs. PTQ:**\n- PTQ drops < 2% F1 → use PTQ (free, no training cost)\n- PTQ drops ≥ 2% F1 → use QAT (3 epochs to recover accuracy)\n\n> ⏱️ **~20–30 minutes on T4 GPU.** Checkpoint saved to `fincompress/checkpoints/student_qat/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e3d48ebcc4687",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 4b-i: Quantization-Aware Training ────────────────────────────────\n# Fine-tunes the INT8-quantized student for 3 epochs with fake-quant nodes.\n# Logs: fincompress/logs/qat_training.csv\n# Output: fincompress/checkpoints/student_qat/\n\n!python -m fincompress.quantization.qat"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c402c9bb2134d17",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 4b-ii: QAT Training Curves ──────────────────────────────────────\n# Dual-axis plot: train loss (left) and val Macro F1 (right) over QAT epochs.\n# F1 should recover toward the FP32 baseline over the 3 fine-tuning epochs.\n\nimport pandas as pd, matplotlib.pyplot as plt, os\n\nlog_df = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'logs', 'qat_training.csv'))\n\nfig, ax1 = plt.subplots(figsize=(9, 5))\nax2 = ax1.twinx()\nax1.plot(log_df['epoch'], log_df['train_loss'], 'o-', color='steelblue', label='Train Loss')\nax2.plot(log_df['epoch'], log_df['val_f1'],     's-', color='coral',     label='Val Macro F1')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Train Loss', color='steelblue'); ax1.tick_params(axis='y', labelcolor='steelblue')\nax2.set_ylabel('Val Macro F1', color='coral');   ax2.tick_params(axis='y', labelcolor='coral')\nax1.set_title('QAT: Training Loss and Val Macro F1')\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\nax1.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df588f7147fe49f2",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 4b-iii: FP32 vs PTQ vs QAT 3-Way Comparison ─────────────────────\n# Compares the three quantization variants.\n# QAT should recover some or all of the F1 lost by PTQ.\n\nimport json, os\n\ndef load_info(ckpt_dir_name):\n    p = os.path.join(PROJECT_PATH, 'fincompress', 'checkpoints', ckpt_dir_name, 'checkpoint_info.json')\n    with open(p) as f: return json.load(f)\n\nfp32_info = load_info('student_intermediate')\nptq_info  = load_info('student_ptq')\nqat_info  = load_info('student_qat')\n\nrows = [('Student (FP32 — Intermediate KD)', fp32_info),\n        ('Student (INT8 — PTQ)',              ptq_info),\n        ('Student (INT8 — QAT)',              qat_info)]\n\nprint(f\"\\n{'Model':<35} {'Val Macro F1':>14} {'Size (MB)':>11}\")\nprint(\"-\" * 63)\nfor name, info in rows:\n    f1   = info.get('best_val_f1', info.get('val_f1', 0))\n    size = info.get('size_mb', 0)\n    print(f\"{name:<35} {f1:>14.4f} {size:>11.1f}\")\n\nfp32_f1 = fp32_info.get('best_val_f1', fp32_info.get('val_f1', 0))\nptq_f1  = ptq_info.get('best_val_f1',  ptq_info.get('val_f1',  0))\nqat_f1  = qat_info.get('best_val_f1',  qat_info.get('val_f1',  0))\nprint(f\"\\n  PTQ F1 change vs FP32: {ptq_f1 - fp32_f1:+.4f}\")\nprint(f\"  QAT F1 change vs FP32: {qat_f1 - fp32_f1:+.4f}\")\nprint(f\"  QAT recovery over PTQ: {qat_f1 - ptq_f1:+.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "af0653c9e81a4c71",
   "metadata": {},
   "source": "---\n## Section 5 — Structured Pruning\n\nPruning removes parameters from a trained model. There are two main approaches:\n\n- **Unstructured pruning:** Zero out individual weights → sparse matrix. Requires specialised sparse-matrix kernels to see real inference speedup (standard hardware doesn't accelerate sparse ops natively).\n- **Structured pruning (our approach):** Remove entire attention heads or FFN neuron groups → smaller, *dense* matrices. Hardware-friendly — no specialised kernels needed.\n\n**Why can we prune so much?** Large pre-trained models are heavily over-parameterised. Research shows 30–50% of BERT attention heads are redundant on downstream classification tasks — some heads attend to similar patterns, others to uninformative tokens like punctuation.\n\n**Head importance scoring (entropy-based):**\n```\nimportance = 1 − normalised_entropy(attention_weights)\n```\nA head that always attends sharply to a few tokens (low entropy, focused) is *important*. A head with near-uniform attention across all tokens (high entropy, diffuse) is *prunable*.\n\n**Iterative prune + recover loop:**\n1. Prune the lowest-importance 10% of heads\n2. Fine-tune for 1 epoch to let remaining heads adapt\n3. Repeat until target sparsity reached\n\nSaves `pruned_teacher_30pct` and `pruned_teacher_50pct` checkpoints.\n\n> ⏱️ **~60–90 minutes on T4 GPU.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7c491cd5845a7",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 5a: Iterative Prune + Recover ────────────────────────────────────\n# Prunes teacher attention heads iteratively (10% at a time) with 1-epoch\n# recovery fine-tuning after each pruning step.\n# Outputs: fincompress/checkpoints/pruned_teacher_30pct/\n#          fincompress/checkpoints/pruned_teacher_50pct/\n#          fincompress/results/pruning_curve.csv\n\n!python -m fincompress.pruning.prune_finetune"
  },
  {
   "cell_type": "markdown",
   "id": "43dcfbe43dab4fe4",
   "metadata": {},
   "source": "**Reading the head importance heatmap below:**\n\nEach cell shows the importance score (0–1) for one attention head in one layer. Brighter = more important. Look for:\n- **Dark columns:** entire heads that are redundant across all layers (safe to prune)\n- **Dark rows:** layers where most heads are dispensable\n- **Bright cells in the final layer:** task-specific heads the model needs for classification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b439016e34253",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 5b: Attention Head Importance Heatmap ────────────────────────────\n# Computes entropy-based importance scores for every attention head in the\n# teacher model, then visualises them as a [layers × heads] heatmap.\n# Brighter = more important (lower entropy = more focused attention).\n\nimport torch, os, numpy as np\nimport pandas as pd, matplotlib.pyplot as plt, seaborn as sns\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import DataLoader, Dataset\n\nclass FinDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.texts  = df['text'].tolist()\n        self.labels = df['label'].tolist()\n        self.tok = tokenizer; self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], max_length=self.max_len,\n                       padding='max_length', truncation=True, return_tensors='pt')\n        return {'input_ids':      enc['input_ids'].squeeze(0),\n                'attention_mask': enc['attention_mask'].squeeze(0),\n                'token_type_ids': enc.get('token_type_ids',\n                    torch.zeros(self.max_len, dtype=torch.long)).squeeze(0),\n                'label':          torch.tensor(self.labels[i])}\n\ndevice    = 'cuda' if torch.cuda.is_available() else 'cpu'\nckpt_dir  = os.path.join(PROJECT_PATH, 'fincompress', 'checkpoints', 'teacher')\ntokenizer = AutoTokenizer.from_pretrained(os.path.join(ckpt_dir, 'tokenizer'))\nteacher   = AutoModelForSequenceClassification.from_pretrained(ckpt_dir).to(device)\nteacher.eval()\n\ndf_val = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'data', 'val.csv'))\nloader = DataLoader(FinDataset(df_val, tokenizer, 128), batch_size=32)\n\nnum_layers = teacher.config.num_hidden_layers\nnum_heads  = teacher.config.num_attention_heads\nimportance = np.zeros((num_layers, num_heads))\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(loader):\n        if batch_idx >= 30: break\n        out = teacher(batch['input_ids'].to(device),\n                      attention_mask=batch['attention_mask'].to(device),\n                      output_attentions=True)\n        for layer_idx, attn in enumerate(out.attentions):\n            eps     = 1e-9\n            entropy = -(attn * (attn + eps).log()).sum(dim=-1).mean(dim=-1)  # [B, heads]\n            max_ent = np.log(batch['input_ids'].shape[1])\n            importance[layer_idx] += (1 - entropy / (max_ent + eps)).mean(dim=0).cpu().numpy()\n\nimportance /= min(30, len(loader))\n\nfig, ax = plt.subplots(figsize=(14, 6))\nsns.heatmap(importance, annot=True, fmt='.2f', cmap='viridis', linewidths=0.5,\n            xticklabels=[f'H{i}' for i in range(num_heads)],\n            yticklabels=[f'L{i}' for i in range(num_layers)], ax=ax)\nax.set_xlabel('Attention Head'); ax.set_ylabel('Layer')\nax.set_title('Attention Head Importance Scores (pre-pruning)\\nBrighter = More Important (higher score = more focused attention)')\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "1e58a1c55a044b4d",
   "metadata": {},
   "source": "**Reading the pruning curve below:**\n\nThe x-axis shows what percentage of attention heads have been removed. The y-axis shows val Macro F1 at that sparsity level. The **red dashed line** marks the \"cliff\" — the point where F1 drops more than 0.02 below its peak. Pruning beyond the cliff causes disproportionate quality loss as important heads start being removed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d4c5763ad44a1",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 5c: Pruning Curve with Cliff Detection ───────────────────────────\n# Plots F1 vs. sparsity. Detects and marks the \"cliff\" — the point where\n# removing more heads starts causing disproportionate accuracy loss.\n\nimport pandas as pd, matplotlib.pyplot as plt, os\n\ncurve_df = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'results', 'pruning_curve.csv'))\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(curve_df['heads_pruned_pct'], curve_df['val_f1'], 'o-',\n        color='steelblue', linewidth=2, markersize=7, label='Val Macro F1')\n\npeak_f1  = curve_df['val_f1'].max()\ncliff_thr = peak_f1 - 0.02\ncliff_row = curve_df[curve_df['val_f1'] < cliff_thr].head(1)\n\nif not cliff_row.empty:\n    cliff_pct = cliff_row['heads_pruned_pct'].values[0]\n    ax.axvline(x=cliff_pct, color='red', linestyle='--', linewidth=1.5,\n               label=f'Cliff @ {cliff_pct:.0f}% pruned')\n    ax.axvspan(cliff_pct, curve_df['heads_pruned_pct'].max(), alpha=0.08, color='red')\n    ax.annotate(f'Cliff ({cliff_pct:.0f}%)', xy=(cliff_pct, cliff_thr),\n                xytext=(cliff_pct + 5, cliff_thr - 0.01),\n                arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=10)\n\nax.axhline(y=peak_f1, color='gray', linestyle=':', alpha=0.5,\n           label=f'Peak F1 = {peak_f1:.3f}')\nax.set_xlabel('Heads Pruned (%)'); ax.set_ylabel('Val Macro F1')\nax.set_title('Structured Pruning Curve — F1 vs. Sparsity')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0293c200b0b404e",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 5d: Pruning Results Table ────────────────────────────────────────\n# Full numeric table: sparsity level, val F1, and delta from peak F1.\n\nimport pandas as pd, os\n\ncurve_df = pd.read_csv(os.path.join(PROJECT_PATH, 'fincompress', 'results', 'pruning_curve.csv'))\npeak_f1  = curve_df['val_f1'].max()\npeak_pct = curve_df.loc[curve_df['val_f1'].idxmax(), 'heads_pruned_pct']\n\nprint(f\"\\n{'Heads Pruned':>14} {'Val Macro F1':>14} {'Δ vs Peak':>12}\")\nprint(\"-\" * 43)\nfor _, row in curve_df.iterrows():\n    delta = row['val_f1'] - peak_f1\n    print(f\"{row['heads_pruned_pct']:>13.0f}% {row['val_f1']:>14.4f} {delta:>12.4f}\")\n\nprint(f\"\\nPeak F1:  {peak_f1:.4f}  @ {peak_pct:.0f}% pruned\")\nprint(f\"Final F1: {curve_df['val_f1'].iloc[-1]:.4f}  @ {curve_df['heads_pruned_pct'].iloc[-1]:.0f}% pruned\")\nprint(f\"Total F1 drop: {curve_df['val_f1'].iloc[-1] - peak_f1:+.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cf60cc8247de4224",
   "metadata": {},
   "source": "**Interpretation:**\n- **Surviving heads** (high importance scores) tend to cluster in lower layers (capturing syntactic structure) and the final layer (task-specific classification signal).\n- **Prunable heads** are often in middle layers — these appear to encode redundant syntactic patterns already captured elsewhere in the model.\n- The **cliff behaviour** is characteristic of structured pruning: the model is robust to losing redundant heads, but once heads carrying unique information are removed, performance collapses rapidly.\n- **30% sparsity** is typically safe for BERT-family models on classification tasks. 50% sparsity often requires more careful recovery fine-tuning."
  },
  {
   "cell_type": "markdown",
   "id": "d51f224034d54cee",
   "metadata": {},
   "source": "---\n## Section 6 — Benchmarking All 7 Variants\n\nNow that all compression techniques are applied, we run a unified benchmark across all 7 model variants:\n1. `teacher` — FinBERT fine-tuned (FP32, 12 layers)\n2. `student_vanilla` — 4-layer student via soft-label KD\n3. `student_intermediate` — 4-layer student via intermediate KD\n4. `student_ptq` — INT8 post-training quantization\n5. `student_qat` — INT8 quantization-aware training\n6. `pruned_teacher_30pct` — Teacher with 30% heads removed\n7. `pruned_teacher_50pct` — Teacher with 50% heads removed\n\n**Metrics:**\n- **Test Macro F1** and accuracy on the held-out test set\n- **Median CPU latency** with 10 warmup samples, then median over 100 samples\n- **Throughput** in samples/second\n\n**Why median latency (not mean)?** CPU latency distributions are right-skewed — rare GC pauses and cache misses create extreme outliers. The median is more representative of typical inference behaviour.\n\n> ⏱️ **~10–15 minutes.** Results saved to `fincompress/results/benchmark_results.json` and `benchmark_results.csv`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99475cf89384365",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 6a: Run Unified Benchmark ────────────────────────────────────────\n# Loads all 7 model checkpoints and evaluates them on the test set.\n# Measures accuracy, Macro F1, median CPU latency, and throughput.\n# Output: fincompress/results/benchmark_results.json + .csv\n\n!python -m fincompress.evaluation.benchmark"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39837aae1d429a",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 6b: Benchmark Results Table ──────────────────────────────────────\n# Loads and pretty-prints the benchmark results CSV.\n\nimport pandas as pd, os\n\nresults_path = os.path.join(PROJECT_PATH, 'fincompress', 'results', 'benchmark_results.csv')\ndf = pd.read_csv(results_path)\n\npriority_cols = ['model', 'accuracy', 'macro_f1', 'size_mb',\n                 'cpu_latency_ms_median', 'throughput_samples_per_sec']\ncols = [c for c in priority_cols if c in df.columns]\n\ndf_show = df[cols].copy()\ndf_show.columns = [c.replace('_', ' ').title() for c in cols]\n\nprint(\"\\n\" + \"=\" * 90)\nprint(\"FinCompress — Final Benchmark Results\")\nprint(\"=\" * 90)\nprint(df_show.to_string(index=False, float_format='{:.4f}'.format))\nprint(\"=\" * 90)"
  },
  {
   "cell_type": "markdown",
   "id": "d216bbf5aae74197",
   "metadata": {},
   "source": "---\n## Section 7 — Analysis & Publication-Quality Plots\n\nWe generate 6 plots that summarise the compression study end-to-end:\n\n| Plot | What it shows |\n|------|--------------|\n| **Pareto** | Accuracy vs. latency trade-off. Bubble size = model size. Top-left corner is better. |\n| **Compression bar** | Size, latency, and F1 normalised to teacher baseline (< 1.0 = compressed) |\n| **Latency distribution** | Box plots of raw latency measurements — reveals spread and outliers |\n| **F1 degradation** | How much Macro F1 each compression technique costs (green < 2%, orange 2–5%, red > 5%) |\n| **Pruning curve** | F1 vs. sparsity with cliff detection |\n| **KD loss curves** | Vanilla vs. intermediate distillation loss components side by side |\n\nAll plots are saved to `fincompress/results/plots/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f440f857d745c4",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7: Plot Setup ─────────────────────────────────────────────────────\n# Load benchmark results and configure plotting environment.\n\nimport os, json\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt, seaborn as sns\nfrom pathlib import Path\n\nsns.set_style('whitegrid')\nFIGSIZE = (10, 6)\nDPI     = 150\n\nRESULTS_DIR = Path(PROJECT_PATH) / 'fincompress' / 'results'\nPLOTS_DIR   = RESULTS_DIR / 'plots'\nPLOTS_DIR.mkdir(parents=True, exist_ok=True)\n\ndf = pd.read_csv(RESULTS_DIR / 'benchmark_results.csv')\nprint(f\"Loaded {len(df)} variants. Plots will be saved to: {PLOTS_DIR}\")\nprint(df[['model', 'macro_f1', 'size_mb']].to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8f4eeb8244c48",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7a: Plot 1 — Pareto Frontier ─────────────────────────────────────\n# Accuracy vs. latency scatter plot. Bubble size = model size.\n# Points toward the top-left are better (higher F1, lower latency).\n\ncolor_map = {\n    'teacher':               '#e63946',\n    'student_vanilla':       '#457b9d',\n    'student_intermediate':  '#1d3557',\n    'student_ptq':           '#2a9d8f',\n    'student_qat':           '#264653',\n    'pruned_teacher_30pct':  '#e9c46a',\n    'pruned_teacher_50pct':  '#f4a261',\n}\n\nfig, ax = plt.subplots(figsize=FIGSIZE)\nfor _, row in df.iterrows():\n    model = row.get('model', '')\n    x     = row.get('cpu_latency_ms_median', row.get('latency_ms', 0))\n    y     = row.get('macro_f1', 0)\n    size  = row.get('size_mb', 10) * 20\n    color = color_map.get(model, '#888888')\n    ax.scatter(x, y, s=size, color=color, alpha=0.85,\n               edgecolors='white', linewidth=1.5, zorder=3)\n    ax.annotate(model.replace('_', '\\n'), (x, y),\n                textcoords='offset points', xytext=(8, 4), fontsize=8)\n\nax.set_xlabel('CPU Latency — Median (ms)')\nax.set_ylabel('Test Macro F1')\nax.set_title('FinCompress: Pareto Frontier — Accuracy vs. Latency\\n(bubble size proportional to model size in MB)')\nax.text(0.98, 0.02, 'Top-left is better', transform=ax.transAxes,\n        ha='right', va='bottom', style='italic', color='gray')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(PLOTS_DIR / 'pareto_plot.png', dpi=DPI, bbox_inches='tight')\nplt.show()\nprint(\"Saved: pareto_plot.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f569894be4837",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7b: Plot 2 — Compression Bar Chart ───────────────────────────────\n# All metrics normalised to teacher = 1.0 baseline.\n# Size ratio < 1 = model is smaller. Latency ratio < 1 = model is faster.\n# F1 ratio should stay as close to 1.0 as possible.\n\nteacher_row = df[df['model'] == 'teacher'].iloc[0]\nt_size = teacher_row.get('size_mb', 1)\nt_lat  = teacher_row.get('cpu_latency_ms_median', teacher_row.get('latency_ms', 1))\nt_f1   = teacher_row.get('macro_f1', 1)\n\nothers = df[df['model'] != 'teacher']\nmodels_list = others['model'].tolist()\nrows_list   = [others[others['model'] == m].iloc[0] for m in models_list]\n\nsize_ratios = [r.get('size_mb', 0) / t_size for r in rows_list]\nlat_ratios  = [r.get('cpu_latency_ms_median', r.get('latency_ms', 0)) / t_lat for r in rows_list]\nf1_ratios   = [r.get('macro_f1', 0) / t_f1 for r in rows_list]\n\nx     = np.arange(len(models_list)); width = 0.25\nfig, ax = plt.subplots(figsize=(14, 6))\nb1 = ax.bar(x - width, size_ratios, width, label='Size ratio',    color='steelblue',     alpha=0.85)\nb2 = ax.bar(x,         lat_ratios,  width, label='Latency ratio', color='coral',          alpha=0.85)\nb3 = ax.bar(x + width, f1_ratios,   width, label='F1 ratio',      color='mediumseagreen', alpha=0.85)\nax.axhline(y=1.0, color='black', linestyle='--', linewidth=1.2, label='Teacher baseline (1.0)')\nax.set_xticks(x)\nax.set_xticklabels([m.replace('_', '\\n') for m in models_list], fontsize=8)\nax.set_ylabel('Ratio relative to teacher')\nax.set_title('Compression Ratios Relative to Teacher\\n(lower size/latency = more compressed; F1 should stay near 1.0)')\nax.legend(); ax.grid(True, axis='y', alpha=0.3)\nplt.tight_layout()\nplt.savefig(PLOTS_DIR / 'compression_bar_chart.png', dpi=DPI, bbox_inches='tight')\nplt.show()\nprint(\"Saved: compression_bar_chart.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74718feb8ed84ab3",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7c: Plot 3 — Latency Distribution Box Plots ──────────────────────\n# Box plots of raw per-sample latency measurements for each model.\n# Red line = median. Boxes show IQR. Dots are outliers.\n# Right-skewed distributions (long upper tail) explain why we use median not mean.\n\nimport json\nwith open(RESULTS_DIR / 'latency_raw.json') as f:\n    latency_raw = json.load(f)\n\nmodel_names   = list(latency_raw.keys())\nall_latencies = [latency_raw[m] for m in model_names]\n\nfig, ax = plt.subplots(figsize=(14, 6))\nbp = ax.boxplot(all_latencies,\n                labels=[m.replace('_', '\\n') for m in model_names],\n                patch_artist=True,\n                medianprops=dict(color='red', linewidth=2),\n                flierprops=dict(marker='o', markerfacecolor='lightgray',\n                                markersize=3, alpha=0.5))\nfor patch in bp['boxes']:\n    patch.set(facecolor='#fff3cd', alpha=0.85)\n\nax.set_ylabel('Latency (ms)')\nax.set_title('CPU Inference Latency Distribution by Model\\n'\n             '(red line = median — preferred over mean due to right-skewed outliers)')\nax.grid(True, axis='y', alpha=0.3)\nplt.tight_layout()\nplt.savefig(PLOTS_DIR / 'latency_distribution.png', dpi=DPI, bbox_inches='tight')\nplt.show()\nprint(\"Saved: latency_distribution.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6b21122cd4d67",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7d: Plot 4 — F1 Degradation ─────────────────────────────────────\n# Horizontal bar chart showing how much each variant drops below the teacher.\n# Green: drop < 2% (acceptable), Orange: 2–5% (moderate), Red: > 5% (significant).\n\nteacher_f1  = df[df['model'] == 'teacher']['macro_f1'].values[0]\nnon_teacher = df[df['model'] != 'teacher'].copy()\nnon_teacher['f1_drop'] = non_teacher['macro_f1'] - teacher_f1\nnon_teacher = non_teacher.sort_values('f1_drop', ascending=False)\n\ndef bar_color(drop):\n    if   drop > -0.02: return '#2a9d8f'   # green\n    elif drop > -0.05: return '#e9c46a'   # orange\n    else:              return '#e63946'   # red\n\ncolors = [bar_color(d) for d in non_teacher['f1_drop'].tolist()]\n\nfig, ax = plt.subplots(figsize=FIGSIZE)\nax.barh(non_teacher['model'], non_teacher['f1_drop'],\n        color=colors, alpha=0.85, edgecolor='white')\nax.axvline(x=0, color='black', linestyle='--', linewidth=1)\nfor i, (_, row) in enumerate(non_teacher.iterrows()):\n    ax.text(row['f1_drop'] - 0.001, i, f\"{row['f1_drop']:+.4f}\",\n            va='center', ha='right', fontsize=9, fontweight='bold')\nax.set_xlabel('F1 Drop vs. Teacher')\nax.set_title('Macro F1 Degradation by Compression Technique\\n'\n             '(green < 2% drop, orange 2–5%, red > 5%)')\nax.grid(True, axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig(PLOTS_DIR / 'f1_degradation_bar.png', dpi=DPI, bbox_inches='tight')\nplt.show()\nprint(\"Saved: f1_degradation_bar.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f6c16c8034a6d",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7e: Plot 5 — Pruning Curve ───────────────────────────────────────\n# Same as Section 5c but saves to the plots directory alongside the other figures.\n\nimport pandas as pd, matplotlib.pyplot as plt\n\ncurve_df  = pd.read_csv(RESULTS_DIR / 'pruning_curve.csv')\npeak_f1   = curve_df['val_f1'].max()\ncliff_thr = peak_f1 - 0.02\n\nfig, ax = plt.subplots(figsize=FIGSIZE)\nax.plot(curve_df['heads_pruned_pct'], curve_df['val_f1'], 'o-',\n        color='steelblue', linewidth=2, markersize=7)\n\ncliff_row = curve_df[curve_df['val_f1'] < cliff_thr].head(1)\nif not cliff_row.empty:\n    cliff_pct = cliff_row['heads_pruned_pct'].values[0]\n    ax.axvline(x=cliff_pct, color='red', linestyle='--',\n               label=f'Cliff @ {cliff_pct:.0f}%')\n    ax.axvspan(cliff_pct, curve_df['heads_pruned_pct'].max(), alpha=0.08, color='red')\n    ax.annotate(f'Cliff ({cliff_pct:.0f}%)', xy=(cliff_pct, cliff_thr),\n                xytext=(cliff_pct + 5, cliff_thr - 0.01),\n                arrowprops=dict(arrowstyle='->', color='red'), color='red')\n\nax.axhline(y=peak_f1, color='gray', linestyle=':', alpha=0.5,\n           label=f'Peak F1 = {peak_f1:.3f}')\nax.set_xlabel('Heads Pruned (%)'); ax.set_ylabel('Val Macro F1')\nax.set_title('Structured Pruning: F1 vs. Sparsity')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(PLOTS_DIR / 'pruning_curve.png', dpi=DPI, bbox_inches='tight')\nplt.show()\nprint(\"Saved: pruning_curve.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36813ff78704252",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7f: Plot 6 — KD Loss Curves (Side-by-Side) ──────────────────────\n# Vanilla KD (left): total, CE, KL loss components.\n# Intermediate KD (right): same + hidden state MSE + attention pattern MSE.\n\nimport pandas as pd, matplotlib.pyplot as plt\nfrom pathlib import Path\n\nLOGS_DIR = Path(PROJECT_PATH) / 'fincompress' / 'logs'\nvanilla_log  = pd.read_csv(LOGS_DIR / 'vanilla_kd_training.csv')\nintermed_log = pd.read_csv(LOGS_DIR / 'intermediate_kd_training.csv')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n\nax1.plot(vanilla_log['epoch'], vanilla_log['train_total_loss'], 'o-', color='steelblue',     label='Total')\nax1.plot(vanilla_log['epoch'], vanilla_log['train_ce_loss'],    's-', color='coral',          label='CE')\nax1.plot(vanilla_log['epoch'], vanilla_log['train_kl_loss'],    '^-', color='mediumseagreen', label='KL (×T²)')\nax1.set_title('Vanilla KD Loss Components')\nax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\nax1.legend(); ax1.grid(True, alpha=0.3)\n\nintermed_styles = [\n    ('train_total_loss',  'o-', 'steelblue',     'Total'),\n    ('train_ce_loss',     's-', 'coral',          'CE'),\n    ('train_kl_loss',     '^-', 'mediumseagreen', 'KL (×T²)'),\n    ('train_hidden_loss', 'D-', 'darkorange',     'Hidden MSE'),\n    ('train_attn_loss',   'v-', 'purple',         'Attention MSE'),\n]\nfor col, style, color, label in intermed_styles:\n    if col in intermed_log.columns:\n        ax2.plot(intermed_log['epoch'], intermed_log[col], style, color=color, label=label)\nax2.set_title('Intermediate KD Loss Components')\nax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss')\nax2.legend(); ax2.grid(True, alpha=0.3)\n\nfig.suptitle('Knowledge Distillation Training Dynamics', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(PLOTS_DIR / 'kd_loss_curves.png', dpi=DPI, bbox_inches='tight')\nplt.show()\nprint(\"Saved: kd_loss_curves.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce857f1f4194e0b",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 7g: Plot Summary ──────────────────────────────────────────────────\n# Verify all 6 plots were written successfully.\n\nfrom pathlib import Path\nplots = ['pareto_plot.png', 'compression_bar_chart.png', 'latency_distribution.png',\n         'f1_degradation_bar.png', 'pruning_curve.png', 'kd_loss_curves.png']\n\nprint(f\"\\n✅ All plots saved to: {PLOTS_DIR}\\n\")\nfor p in plots:\n    exists = (PLOTS_DIR / p).exists()\n    print(f\"  {'✓' if exists else '✗'}  {p}\")"
  },
  {
   "cell_type": "markdown",
   "id": "e4a85fb4edf14117",
   "metadata": {},
   "source": "---\n## Section 8 — Export Checkpoints\n\nPackage all trained checkpoints into a ZIP for download to your local machine. You need them locally to:\n\n- Run the **local CPU benchmark** with accurate hardware timings: `python3 -m fincompress.evaluation.benchmark`\n- Launch the **Gradio demo** (side-by-side model comparison): `python3 -m fincompress.demo.app` → http://localhost:7860\n\nThe ZIP is saved to your Google Drive root as `fincompress_checkpoints.zip`.\n\n**To download:** open [drive.google.com](https://drive.google.com), right-click `fincompress_checkpoints.zip` → Download.\n\n**To extract locally:** unzip into `FinCompress/fincompress/` — the `checkpoints/` folder should land at `FinCompress/fincompress/checkpoints/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd59099b77f4965",
   "metadata": {},
   "outputs": [],
   "source": "# ── Section 8: Export Checkpoints to ZIP ─────────────────────────────────────\n# Creates fincompress_checkpoints.zip in your Google Drive root.\n# Download it from drive.google.com to use models locally.\n\nimport shutil, os\n\nCHECKPOINTS_DIR = os.path.join(PROJECT_PATH, 'fincompress', 'checkpoints')\nZIP_PATH        = '/content/drive/MyDrive/fincompress_checkpoints'\n\nprint(\"Zipping checkpoints (may take a few minutes for large files)...\")\nshutil.make_archive(ZIP_PATH, 'zip', CHECKPOINTS_DIR)\n\nzip_file = ZIP_PATH + '.zip'\nsize_mb  = os.path.getsize(zip_file) / 1e6\nprint(f\"\\n✅ Archive created: {zip_file}\")\nprint(f\"   Size: {size_mb:.1f} MB\")\nprint()\nprint(\"Next steps:\")\nprint(\"  1. Open drive.google.com → find fincompress_checkpoints.zip → Download\")\nprint(\"  2. Extract to:  ~/Desktop/UMD - MSML/Sem 4/FinCompress/fincompress/\")\nprint(\"  3. Run locally: python3 -m fincompress.evaluation.benchmark\")\nprint(\"  4. Run demo:    python3 -m fincompress.demo.app\")\nprint(\"     Then open:   http://localhost:7860\")"
  }
 ]
}