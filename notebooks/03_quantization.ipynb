{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-03-01",
   "metadata": {},
   "source": [
    "# FinCompress — Notebook 03: Quantization\n",
    "\n",
    "**RUN ON: Colab/GPU or Local/CPU**\n",
    "\n",
    "## Section A: Post-Training Quantization (PTQ)\n",
    "\n",
    "### How PTQ Works\n",
    "\n",
    "Quantization reduces weight precision from FP32 (32-bit float, ~10⁷ values per weight) to INT8 (8-bit integer, 256 values per weight). This gives:\n",
    "- **4× size reduction** (4 bytes → 1 byte per weight)\n",
    "- **2-4× latency improvement** on x86 CPUs with INT8 SIMD instructions\n",
    "\n",
    "**How observers work:** PyTorch's `prepare()` inserts observer modules that record activation ranges (min/max or histogram) during a calibration pass. These ranges determine the quantization *scale* and *zero-point*: the two values that map INT8 [−128, 127] to the observed FP32 range.\n",
    "\n",
    "**Why fbgemm?** fbgemm is optimized for x86 CPUs (Intel/AMD). qnnpack is for ARM mobile (Android/iOS). Using the wrong backend gives misleadingly slow benchmarks.\n",
    "\n",
    "**Why keep first/last layers in FP32?**\n",
    "- Embedding layers: Quantization error propagates multiplicatively through every subsequent layer\n",
    "- Final classifier: Only 3 outputs — even tiny rounding can change the argmax"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-03-02",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys\n",
    "PROJECT_PATH = '/content/drive/MyDrive/fincompress'  # adjust if running locally\n",
    "os.chdir(PROJECT_PATH)\n",
    "sys.path.insert(0, PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-03-03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PTQ runs on CPU — CUDA does not support static quantization\n",
    "!python -m fincompress.quantization.ptq"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-03-04",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "inter_info = json.loads(Path('fincompress/checkpoints/student_intermediate_kd/checkpoint_info.json').read_text())\n",
    "ptq_info   = json.loads(Path('fincompress/checkpoints/student_ptq/checkpoint_info.json').read_text())\n",
    "\n",
    "print('PTQ Results')\n",
    "print('=' * 48)\n",
    "print(f'{\"Metric\":<25} {\"FP32 Student\":>10} {\"INT8 PTQ\":>10}')\n",
    "print('-' * 48)\n",
    "print(f'{\"Val Macro F1\":<25} {inter_info[\"val_macro_f1\"]:>10.4f} {ptq_info[\"val_macro_f1\"]:>10.4f}')\n",
    "print(f'{\"Size (MB)\":<25} {inter_info[\"size_mb\"]:>10.1f} {ptq_info[\"size_mb\"]:>10.1f}')\n",
    "ratio = inter_info['size_mb'] / ptq_info['size_mb']\n",
    "f1_drop = inter_info['val_macro_f1'] - ptq_info['val_macro_f1']\n",
    "print('-' * 48)\n",
    "print(f'{\"Compression ratio\":<25} {\"1.0×\":>10} {ratio:.1f}×')\n",
    "print(f'{\"F1 drop\":<25} {\"0.0000\":>10} {f1_drop:>+10.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-03-05",
   "metadata": {},
   "source": [
    "## Section B: Quantization-Aware Training (QAT)\n",
    "\n",
    "### PTQ vs. QAT: Conceptual Difference\n",
    "\n",
    "| | PTQ | QAT |\n",
    "|-|-----|-----|\n",
    "| When applied | After training | During training |\n",
    "| Quantization in forward pass | No (only during calibration) | Yes (fake-quant nodes) |\n",
    "| Backward pass through quant | N/A | Straight-through estimator |\n",
    "| Optimizer awareness | No | Yes — weights adapt to quantization noise |\n",
    "| Training cost | None | 3 additional fine-tuning epochs |\n",
    "| Typical accuracy recovery | Baseline → −2-5% | Baseline → −0.5-2% |\n",
    "\n",
    "**Straight-through estimator (STE):** The rounding function has zero gradient almost everywhere. STE bypasses this by using the identity function as the gradient during the backward pass — pretending quantization is a linear operation. This allows gradient-based optimization through a fundamentally discrete operation.\n",
    "\n",
    "**When is QAT worth the extra training?** When PTQ drops >2% accuracy and you have training data. If PTQ gives acceptable accuracy, skip QAT to save compute."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-03-06",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# QAT training (runs on GPU in Colab)\n",
    "!python -m fincompress.quantization.qat"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-03-07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "log_df = pd.read_csv('fincompress/logs/qat_training.csv')\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(log_df['epoch'], log_df['train_loss'], 'o-', color='steelblue', label='Train Loss')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(log_df['epoch'], log_df['val_f1'], 's-', color='coral', label='Val F1')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss', color='steelblue')\n",
    "ax2.set_ylabel('Val Macro F1', color='coral')\n",
    "ax.set_title('QAT Training Curve')\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-03-08",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "inter_info = json.loads(Path('fincompress/checkpoints/student_intermediate_kd/checkpoint_info.json').read_text())\n",
    "ptq_info   = json.loads(Path('fincompress/checkpoints/student_ptq/checkpoint_info.json').read_text())\n",
    "qat_info   = json.loads(Path('fincompress/checkpoints/student_qat/checkpoint_info.json').read_text())\n",
    "\n",
    "print('Three-Way Quantization Comparison')\n",
    "print('=' * 60)\n",
    "print(f'{\"Metric\":<25} {\"FP32\":>10} {\"INT8 PTQ\":>10} {\"INT8 QAT\":>10}')\n",
    "print('-' * 60)\n",
    "for info, name in [(inter_info, 'FP32'), (ptq_info, 'PTQ'), (qat_info, 'QAT')]:\n",
    "    pass  # populated below\n",
    "rows = [\n",
    "    ('Val Macro F1', 'val_macro_f1', '.4f'),\n",
    "    ('Size (MB)',    'size_mb',      '.1f'),\n",
    "]\n",
    "for label, key, fmt in rows:\n",
    "    vals = [inter_info[key], ptq_info[key], qat_info[key]]\n",
    "    print(f'{label:<25} {vals[0]:>10{fmt}} {vals[1]:>10{fmt}} {vals[2]:>10{fmt}}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-03-09",
   "metadata": {},
   "source": [
    "## Section C: Production Trade-off Analysis\n",
    "\n",
    "**Which quantization variant would you deploy in production?**\n",
    "\n",
    "| Constraint | Deploy | Reason |\n",
    "|-----------|--------|--------|\n",
    "| Latency SLA ≤ 5ms, accuracy loss ≤2% | INT8 QAT | Best accuracy at INT8 precision |\n",
    "| Zero additional training budget | INT8 PTQ | No retraining required |\n",
    "| Accuracy drop >2% in PTQ | INT8 QAT | 3 epochs recovers most PTQ loss |\n",
    "| Serving on ARM mobile (iOS/Android) | INT8 PTQ with qnnpack backend | Different SIMD target |\n",
    "\n",
    "**Rule of thumb:** Start with PTQ. If accuracy drop is <2%, ship it. If >2%, run QAT."
   ]
  }
 ]
}
