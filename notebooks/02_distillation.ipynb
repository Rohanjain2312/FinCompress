{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-02-01",
   "metadata": {},
   "source": [
    "# FinCompress — Notebook 02: Knowledge Distillation\n",
    "\n",
    "**RUN ON: Colab/GPU**\n",
    "\n",
    "## Section A: Vanilla Knowledge Distillation\n",
    "\n",
    "### How Knowledge Distillation Works\n",
    "\n",
    "Standard supervised learning trains the student on hard labels: a one-hot vector that says \"this is positive\" with probability 1. But the teacher knows more: it assigns probabilities to *all* classes. For a borderline negative sentence, the teacher might output `[0.65, 0.30, 0.05]` — the 0.30 neutral probability tells the student this is ambiguous. Hard labels would just say `[1, 0, 0]` and discard this nuance entirely.\n",
    "\n",
    "**Temperature T** controls how soft the teacher's distribution is:\n",
    "- T=1: Original softmax — the teacher's actual probability distribution\n",
    "- T>1: Softer — minority classes get more relative probability, amplifying the information in dark knowledge\n",
    "- T→∞: Uniform — all information is lost\n",
    "\n",
    "**T² scaling** is required because KL divergence between two T-softened distributions scales as 1/T². Without the correction, high temperature would make the KL loss negligible relative to CE loss.\n",
    "\n",
    "**α (alpha)** balances soft-label KL loss vs. hard-label CE loss. α=0.5 gives equal weight."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-02",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys\n",
    "PROJECT_PATH = '/content/drive/MyDrive/fincompress'\n",
    "os.chdir(PROJECT_PATH)\n",
    "sys.path.insert(0, PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run vanilla KD distillation\n",
    "!python -m fincompress.distillation.soft_label_distillation"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-04",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_df = pd.read_csv('fincompress/logs/vanilla_kd_training.csv')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(log_df['epoch'], log_df['train_total_loss'], 'o-', label='Total Loss', color='steelblue')\n",
    "ax.plot(log_df['epoch'], log_df['train_ce_loss'],    's-', label='CE Loss',    color='coral')\n",
    "ax.plot(log_df['epoch'], log_df['train_kl_loss'],    '^-', label='KL Loss',    color='mediumseagreen')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Vanilla KD Training Loss Components')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-05",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "teacher_info = json.loads(Path('fincompress/checkpoints/teacher/checkpoint_info.json').read_text())\n",
    "student_info = json.loads(Path('fincompress/checkpoints/student_vanilla_kd/checkpoint_info.json').read_text())\n",
    "\n",
    "print('=' * 55)\n",
    "print(f'{\"Metric\":<25} {\"Teacher\":>12} {\"Vanilla KD\":>12}')\n",
    "print('-' * 55)\n",
    "print(f'{\"Val Macro F1\":<25} {teacher_info[\"val_macro_f1\"]:>12.4f} {student_info[\"val_macro_f1\"]:>12.4f}')\n",
    "print(f'{\"Params\":<25} {teacher_info[\"num_parameters\"]:>12,} {student_info[\"num_parameters\"]:>12,}')\n",
    "print(f'{\"Size (MB)\":<25} {teacher_info[\"size_mb\"]:>12.1f} {student_info[\"size_mb\"]:>12.1f}')\n",
    "compression = teacher_info['num_parameters'] / student_info['num_parameters']\n",
    "f1_gap = teacher_info['val_macro_f1'] - student_info['val_macro_f1']\n",
    "print('-' * 55)\n",
    "print(f'{\"Compression ratio\":<25} {1.0:>12.1f}× {compression:>11.1f}×')\n",
    "print(f'{\"F1 gap from teacher\":<25} {0.0:>12.4f} {f1_gap:>+12.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-06",
   "metadata": {},
   "source": [
    "## Section B: Intermediate Layer Distillation\n",
    "\n",
    "### Why Logit-Level Supervision Alone Is Insufficient\n",
    "\n",
    "Vanilla KD tells the student: \"your output distribution should look like mine.\" But there are many internal computation paths that produce similar output distributions. Some of these paths are shortcuts that happen to work on training data but fail on distribution shifts.\n",
    "\n",
    "**Intermediate distillation** adds two constraints:\n",
    "\n",
    "1. **Hidden state MSE**: The student's layer outputs (projected to teacher dimension) should match the teacher's layer outputs at corresponding depths. This forces the student's *representations* to be similar, not just its final predictions.\n",
    "\n",
    "2. **Attention pattern MSE**: The student's attention weight matrices should match the teacher's at corresponding layers. This constrains the student's *routing decisions* — which tokens it attends to — to follow the teacher's proven strategy.\n",
    "\n",
    "**Layer mapping** {0→2, 1→5, 2→8, 3→11} maps each of the 4 student layers to teacher layers spaced every 3 positions across the 12-layer hierarchy. This ensures supervision at early (syntactic), middle (semantic), and final (task-relevant) abstraction levels."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run intermediate KD distillation\n",
    "!python -m fincompress.distillation.intermediate_distillation"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-08",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_df = pd.read_csv('fincompress/logs/intermediate_kd_training.csv')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "colors = {'train_total_loss': 'steelblue', 'train_ce_loss': 'coral',\n",
    "          'train_kl_loss': 'mediumseagreen', 'train_hidden_loss': 'darkorange', 'train_attn_loss': 'purple'}\n",
    "labels = {'train_total_loss': 'Total', 'train_ce_loss': 'CE', 'train_kl_loss': 'KL',\n",
    "          'train_hidden_loss': 'Hidden MSE', 'train_attn_loss': 'Attn MSE'}\n",
    "\n",
    "for col, color in colors.items():\n",
    "    ax.plot(log_df['epoch'], log_df[col], 'o-', label=labels[col], color=color)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Intermediate KD — All Loss Components')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-09",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "vanilla_info = json.loads(Path('fincompress/checkpoints/student_vanilla_kd/checkpoint_info.json').read_text())\n",
    "inter_info   = json.loads(Path('fincompress/checkpoints/student_intermediate_kd/checkpoint_info.json').read_text())\n",
    "\n",
    "print('=' * 55)\n",
    "print(f'{\"Metric\":<25} {\"Vanilla KD\":>12} {\"Intermediate KD\":>15}')\n",
    "print('-' * 55)\n",
    "print(f'{\"Val Macro F1\":<25} {vanilla_info[\"val_macro_f1\"]:>12.4f} {inter_info[\"val_macro_f1\"]:>15.4f}')\n",
    "print(f'{\"Size (MB)\":<25} {vanilla_info[\"size_mb\"]:>12.1f} {inter_info[\"size_mb\"]:>15.1f}')\n",
    "delta = inter_info['val_macro_f1'] - vanilla_info['val_macro_f1']\n",
    "print('-' * 55)\n",
    "print(f'{\"F1 improvement\":<25} {\"\":>12} {delta:>+15.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-10",
   "metadata": {},
   "source": [
    "## Section C: Comparison and Analysis\n",
    "\n",
    "### When Would You Choose Vanilla KD Over Intermediate KD in Practice?\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Teacher is a black-box API (no internal access) | Vanilla KD — you only have logits |\n",
    "| Limited compute budget | Vanilla KD — no projection layers, faster training |\n",
    "| Student architecture differs significantly from teacher | Vanilla KD — hidden state MSE assumes compatible representations |\n",
    "| Maximum accuracy with full teacher access | Intermediate KD — consistently outperforms vanilla by 1-3 F1 points |\n",
    "| Production: latency SLA requires smallest model | Intermediate KD — the extra training cost pays off in better student quality |"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-02-11",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "teacher_info = json.loads(Path('fincompress/checkpoints/teacher/checkpoint_info.json').read_text())\n",
    "vanilla_info = json.loads(Path('fincompress/checkpoints/student_vanilla_kd/checkpoint_info.json').read_text())\n",
    "inter_info   = json.loads(Path('fincompress/checkpoints/student_intermediate_kd/checkpoint_info.json').read_text())\n",
    "\n",
    "models = ['Teacher', 'Vanilla KD', 'Intermediate KD']\n",
    "infos  = [teacher_info, vanilla_info, inter_info]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "metrics = ['val_macro_f1', 'num_parameters', 'size_mb']\n",
    "titles  = ['Val Macro F1', 'Parameters', 'Size (MB)']\n",
    "colors  = ['#888888', '#4472C4', '#2196F3']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    values = [info[metric] for info in infos]\n",
    "    bars = ax.bar(models, values, color=colors)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(title)\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.01,\n",
    "                f'{val:.4f}' if isinstance(val, float) else f'{val:,}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Teacher vs. Distilled Students: Key Metrics', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ]
}
