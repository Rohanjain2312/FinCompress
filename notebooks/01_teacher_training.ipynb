{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01-01",
   "metadata": {},
   "source": [
    "# FinCompress — Notebook 01: Teacher Training\n",
    "\n",
    "**RUN ON: Colab/GPU**\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "FinCompress is a practitioner's study in compressing a domain-pretrained language model for production inference. We start with **ProsusAI/FinBERT** — a BERT-base model further pre-trained on financial text — as our teacher, then compress it three ways:\n",
    "\n",
    "1. **Knowledge Distillation** (Notebooks 01–02): Train a 4-layer student to mimic the teacher\n",
    "2. **INT8 Quantization** (Notebook 03): Reduce weight precision from FP32 to INT8\n",
    "3. **Structured Pruning** (Notebook 04): Remove entire attention heads and FFN neurons\n",
    "\n",
    "All variants are benchmarked identically in Notebook 05.\n",
    "\n",
    "## Why Start with FinBERT?\n",
    "\n",
    "BERT-base was pre-trained on Wikipedia and BookCorpus — general English text. Financial text has a very different distribution: domain-specific vocabulary (\"EPS beat\", \"EBITDA\", \"margin compression\"), numerical reasoning, and hedged language (\"may adversely affect\", \"subject to regulatory approval\"). FinBERT was further pre-trained on financial news, earnings calls, and analyst reports, giving it:\n",
    "\n",
    "- Better tokenization of financial terms (they appear as full tokens, not subwords)\n",
    "- Contextualized representations that already encode financial sentiment signals\n",
    "- Higher baseline accuracy on financial NLP tasks *before* any task-specific fine-tuning\n",
    "\n",
    "Starting with a domain-aware teacher gives the student a richer target to distill — the teacher's soft labels encode more domain-relevant uncertainty structure than BERT-base's would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-01-02",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "## Setup\n\n**Run the Setup cell below first** (mounts Drive, syncs the repo, installs dependencies). All three steps are combined so the pip install always uses the correct path."
  },
  {
   "cell_type": "markdown",
   "id": "cell-01-03",
   "metadata": {},
   "source": [
    "## Google Drive Setup\n",
    "\n",
    "Mount your Google Drive and clone/import the FinCompress repository:\n",
    "\n",
    "```\n",
    "# Recommended Drive structure:\n",
    "# MyDrive/\n",
    "# └── fincompress/\n",
    "#     ├── fincompress/          ← the Python package\n",
    "#     │   ├── data/\n",
    "#     │   ├── checkpoints/\n",
    "#     │   └── ...\n",
    "#     └── requirements_colab.txt\n",
    "```\n",
    "\n",
    "After mounting, `cd` into the project root so relative paths work correctly."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-01-04",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# ── Setup: mount Drive, sync repo, install deps ──────────────────────────────\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nimport sys, os\n\nPROJECT_PATH = '/content/drive/MyDrive/fincompress'\n\nif os.path.exists(os.path.join(PROJECT_PATH, '.git')):\n    print(\"Repo already exists — pulling latest...\")\n    os.system(f'git -C {PROJECT_PATH} pull')\nelse:\n    print(\"Cloning fresh...\")\n    os.makedirs(PROJECT_PATH, exist_ok=True)\n    os.chdir('/content/drive/MyDrive')\n    os.system('git clone https://github.com/Rohanjain2312/FinCompress.git fincompress')\n\nos.chdir(PROJECT_PATH)\nsys.path.insert(0, PROJECT_PATH)\nprint('Working directory:', os.getcwd())\n\n# Install dependencies using absolute path (drive must be mounted first).\n# numpy/pandas/scipy/matplotlib/seaborn/scikit-learn are intentionally NOT\n# pinned — Colab's pre-installed versions are mutually compatible; pinning\n# numpy downgrades it and breaks pre-compiled C extensions (numpy.dtype error).\nos.system('pip install -r /content/drive/MyDrive/fincompress/requirements_colab.txt -q')\nprint(\"Dependencies installed.\")\nprint()\nprint(\"If this is your FIRST run on this runtime:\")\nprint(\"  Runtime -> Restart session -> then re-run ALL cells from the top.\")\nprint(\"  (Required so Colab reloads torch/transformers into the fresh process.)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-01-05",
   "metadata": {},
   "source": [
    "## Dataset: FinancialPhraseBank + FiQA-2018\n",
    "\n",
    "We combine two complementary datasets:\n",
    "\n",
    "| Dataset | Source | Size | Labels |\n",
    "|---------|--------|------|--------|\n",
    "| FinancialPhraseBank (allagree) | Expert-annotated financial news | ~2,264 | {negative, neutral, positive} |\n",
    "| FiQA-2018 Sentiment | Crowdsourced financial Q&A | ~1,174 | Continuous [-1, 1] → discretized |\n",
    "\n",
    "**Why combine?** FinancialPhraseBank has high-quality expert labels but is small (~2K samples). FiQA provides additional diversity but uses continuous scores that we threshold at ±0.1. Combined and de-duplicated, we get a richer training set.\n",
    "\n",
    "**Class distribution rationale:** Financial news skews neutral (most corporate announcements are factual). We use stratified splits to preserve this distribution across train/val/test — testing on a different distribution than training would give misleadingly optimistic F1."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-01-06",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare dataset (downloads from HuggingFace Hub)\n",
    "!python -m fincompress.data.prepare_dataset\n",
    "\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv('fincompress/data/train.csv')\n",
    "df_val   = pd.read_csv('fincompress/data/val.csv')\n",
    "df_test  = pd.read_csv('fincompress/data/test.csv')\n",
    "\n",
    "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "for split_name, df in [('train', df_train), ('val', df_val), ('test', df_test)]:\n",
    "    print(f'\\n{split_name} ({len(df)} samples):')\n",
    "    print(df['label'].map(label_map).value_counts(normalize=True).round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-01-07",
   "metadata": {},
   "source": [
    "## Teacher Fine-Tuning\n",
    "\n",
    "We fine-tune FinBERT on our combined dataset using a manual PyTorch training loop (no HuggingFace Trainer). Key choices:\n",
    "\n",
    "- **AdamW** with weight decay 0.01 (bias/LayerNorm excluded from decay)\n",
    "- **Linear warmup** for 10% of total steps, then linear decay — prevents large gradient updates before representations stabilize\n",
    "- **Early stopping** after 3 consecutive non-improving epochs — saves compute and prevents overfitting\n",
    "- **Target: val Macro F1 ≥ 0.87** — below this threshold, the teacher's soft labels carry insufficient information for effective distillation\n",
    "\n",
    "Why Macro F1 (not accuracy)? The dataset has class imbalance (neutral >> negative). Macro F1 weights each class equally, penalizing a model that ignores the minority class."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-01-08",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train teacher (logs to checkpoints/teacher/ and logs/teacher_training.csv)\n",
    "!python -m fincompress.teacher.train_teacher"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-01-09",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_df = pd.read_csv('fincompress/logs/teacher_training.csv')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(log_df['epoch'], log_df['train_loss'], 'o-', label='Train Loss', color='steelblue')\n",
    "axes[0].plot(log_df['epoch'], log_df['val_loss'], 's-', label='Val Loss', color='coral')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "axes[0].set_title('Teacher Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(log_df['epoch'], log_df['val_f1'], 'o-', color='mediumseagreen')\n",
    "axes[1].axhline(y=0.87, color='red', linestyle='--', label='Target F1 = 0.87')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Val Macro F1')\n",
    "axes[1].set_title('Teacher Val Macro F1 by Epoch')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Best val F1: {log_df['val_f1'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-01-10",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SimpleDS(Dataset):\n",
    "    def __init__(self, df, tok, maxlen):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tok = tok; self.maxlen = maxlen\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(self.texts[i], max_length=self.maxlen, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {'input_ids': enc['input_ids'].squeeze(0), 'attention_mask': enc['attention_mask'].squeeze(0), 'label': torch.tensor(self.labels[i])}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained('fincompress/checkpoints/teacher/tokenizer')\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained('fincompress/checkpoints/teacher').to(device)\n",
    "teacher.eval()\n",
    "\n",
    "df_val = pd.read_csv('fincompress/data/val.csv')\n",
    "loader = DataLoader(SimpleDS(df_val, tokenizer, 128), batch_size=64)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        out = teacher(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        all_preds.extend(out.logits.argmax(dim=-1).cpu().tolist())\n",
    "        all_labels.extend(batch['label'].tolist())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "label_names = ['negative', 'neutral', 'positive']\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_names, yticklabels=label_names, cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Teacher Confusion Matrix (Val Set)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-01-11",
   "metadata": {},
   "source": [
    "## Confusion Matrix Interpretation\n",
    "\n",
    "The confusion matrix reveals which sentiment classes are hardest to distinguish:\n",
    "\n",
    "- **Negative ↔ Neutral**: The most common error. Many negative financial statements use hedged or understated language (\"results were below expectations\") that sits close to the neutral boundary in representation space.\n",
    "- **Positive ↔ Neutral**: Less common. Positive announcements tend to use more explicit language (\"record revenues\", \"exceeds estimates\") that is more distinctly positive.\n",
    "- **Negative ↔ Positive**: Rare. Direct sentiment opposition is usually linguistically unambiguous.\n",
    "\n",
    "This confusion pattern tells us what to watch for in the student: if distillation degrades accuracy, it will first affect negative recall."
   ]
  }
 ]
}